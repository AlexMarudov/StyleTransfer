{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.models import vgg16, vgg19\n",
    "from torchvision import transforms\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Training on GPU!\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Training on CPU :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.out_channels,\n",
    "                               kernel_size = 3)\n",
    "        self.batch_norm = nn.InstanceNorm2d(self.out_channels, affine=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolution\n",
    "        orig_x = x.clone()\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Second convolution\n",
    "        x = self.conv(x)\n",
    "        x = self.batch_norm(x)\n",
    "        \n",
    "        # Now add the original to the new one (and use center cropping)\n",
    "        # Calulate the different between the size of each feature (in terms \n",
    "        # of height/width) to get the center of the original feature\n",
    "        height_diff = orig_x.size()[2] - x.size()[2]\n",
    "        width_diff = orig_x.size()[3] - x.size()[3]\n",
    "        \n",
    "        # Add the original to the new (complete the residual block)\n",
    "        x = x + orig_x[:, :,\n",
    "                                 height_diff//2:(orig_x.size()[2] - height_diff//2), \n",
    "                                 width_diff//2:(orig_x.size()[3] - width_diff//2)]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransformationNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageTransformationNetwork, self).__init__()\n",
    "        # Use reflection padding to keep the end shape\n",
    "        self.ref_pad = nn.ReflectionPad2d(40)\n",
    "        \n",
    "        # Initial convolutions\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3,\n",
    "                               out_channels = 32,\n",
    "                               kernel_size = 9,\n",
    "                               padding = 6,\n",
    "                               padding_mode = 'reflect')\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 32,\n",
    "                               out_channels = 64,\n",
    "                               kernel_size = 3,\n",
    "                               stride = 2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 64,\n",
    "                               out_channels = 128,\n",
    "                               kernel_size = 3,\n",
    "                               stride = 2)\n",
    "        \n",
    "        # Residual Blocks\n",
    "        self.resblock1 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        self.resblock2 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        self.resblock3 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        self.resblock4 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        self.resblock5 = ResidualBlock(in_channels = 128,\n",
    "                                       out_channels = 128)\n",
    "        \n",
    "        # Transpose convoltutions\n",
    "        self.trans_conv1 = nn.ConvTranspose2d(in_channels=128,\n",
    "                                             out_channels=64,\n",
    "                                             kernel_size=2,\n",
    "                                             stride=2)\n",
    "        \n",
    "        self.trans_conv2 = nn.ConvTranspose2d(in_channels=64,\n",
    "                                              out_channels=32,\n",
    "                                              kernel_size=2,\n",
    "                                              stride=2)\n",
    "        \n",
    "        # End with one last convolution\n",
    "        self.conv4 = nn.Conv2d(in_channels = 32,\n",
    "                               out_channels = 3,\n",
    "                               kernel_size = 9,\n",
    "                               padding = 4,\n",
    "                               padding_mode = 'reflect')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply reflection padding\n",
    "        x = self.ref_pad(x)\n",
    "        \n",
    "        # Apply the initial convolutions\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        # Apply the residual blocks\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.resblock4(x)\n",
    "        x = self.resblock5(x)        \n",
    "        \n",
    "        #  Apply the transpose convolutions\n",
    "        x = self.trans_conv1(x)\n",
    "        x = self.trans_conv2(x)\n",
    "        \n",
    "        # Apply the final convolution\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to confirm the residual network works\n",
    "resblock = ResidualBlock(128, 128)\n",
    "test = torch.randn(2, 128, 84, 84)\n",
    "out = resblock(test)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to confirm the transormational network works\n",
    "transformation_net = ImageTransformationNetwork()\n",
    "test = torch.randn(2, 3, 256, 256)\n",
    "out = transformation_net(test)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardVGG19(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ForwardVGG19, self).__init__()\n",
    "        features = list(vgg19(pretrained=True).features)\n",
    "        self.features = nn.ModuleList(features).eval()\n",
    "        \n",
    "    def forward(self, x, style):\n",
    "        results = []\n",
    "        for i, model in enumerate(self.features):\n",
    "            x = model(x)\n",
    "            if style:\n",
    "                if i in {3, 8, 15, 22}:\n",
    "                    results.append(x)\n",
    "            \n",
    "            else:\n",
    "                if i == 15:\n",
    "                    results.append(x)\n",
    "        \n",
    "        return results\n",
    "\n",
    "forward_vgg = ForwardVGG19().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_batch(batch):\n",
    "    \"\"\"\n",
    "    Before we send an image into the VGG19, we have to normalize it\n",
    "    \"\"\"\n",
    "    # Define the means and standard deviations\n",
    "    vgg_means = [0.485, 0.456, 0.406]\n",
    "    vgg_std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    # Clone the batch to make changes to it\n",
    "    ret = batch.clone()\n",
    "    \n",
    "    # Normalize to between 0 and 255 (input image is 255-value images, not floats)\n",
    "    ret = ret / 255.0\n",
    "    \n",
    "    # Subtract the means and divide by the standard deviations\n",
    "    ret[:, 0, :, :] = (ret[:, 0, :, :] - vgg_means[0]) / vgg_std[0]\n",
    "    ret[:, 1, :, :] = (ret[:, 1, :, :] - vgg_means[1]) / vgg_std[1]\n",
    "    ret[:, 2, :, :] = (ret[:, 2, :, :] - vgg_means[2]) / vgg_std[2]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_batch(batch):\n",
    "    vgg_means = [123.68, 116.779, 103.94]\n",
    "    ret = torch.zeros(*batch.size())\n",
    "    ret[:, 0, :, :] = batch[:, 0, :, :] + vgg_means[0]\n",
    "    ret[:, 1, :, :] = batch[:, 1, :, :] + vgg_means[1]\n",
    "    ret[:, 2, :, :] = batch[:, 2, :, :] + vgg_means[2]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(batch):\n",
    "    \"\"\"\n",
    "    For the input image, we have to add noise so that the loss between the content image and \n",
    "    input image is not 0\n",
    "    \"\"\"\n",
    "    mean = 0.0\n",
    "    std = 10.0\n",
    "    ret = batch + np.random.normal(mean, std, batch.shape)\n",
    "    ret = np.clip(batch, 0, 255)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gram(matrix):\n",
    "    \"\"\"\n",
    "    Computes the gram matrix\n",
    "    \"\"\"\n",
    "    batches, channels, height, width = matrix.size()\n",
    "    return (1/(channels * height * width)) * (torch.matmul(matrix.view(batches, channels, -1),\n",
    "                                                torch.transpose(matrix.view(batches, channels, -1), 1, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_cost(input, target):\n",
    "    # First normalize both the input and target (preprocess for VGG16)\n",
    "    input_norm = normalize_batch(input)\n",
    "    target_norm = normalize_batch(target)\n",
    "\n",
    "    input_layers = forward_vgg(input_norm, False)\n",
    "    target_layers = forward_vgg(target_norm, False)\n",
    "\n",
    "    accumulated_loss = 0\n",
    "    for layer in range(len(input_layers)):\n",
    "        batches, channels, height, width = input_layers[layer].size()\n",
    "        accumulated_loss = accumulated_loss + torch.mean(torch.square(input_layers[layer] - target_layers[layer]))\n",
    "    \n",
    "    return accumulated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_cost(input, target):\n",
    "    # First normalize both the input and target (preprocess for VGG16)\n",
    "    input_norm = normalize_batch(input)\n",
    "    target_norm = normalize_batch(target)\n",
    "\n",
    "    input_layers = forward_vgg(input_norm, True)\n",
    "    target_layers = forward_vgg(target_norm, True)\n",
    "    \n",
    "    # layer weights\n",
    "    layer_weights = [0.3, 0.7, 0.7, 0.3]\n",
    "    \n",
    "    # The accumulated losses for the style\n",
    "    accumulated_loss = 0\n",
    "    for layer in range(len(input_layers)):\n",
    "        batches, channels, height, width = input_layers[layer].size()\n",
    "        accumulated_loss = accumulated_loss + layer_weights[layer] * \\\n",
    "                            torch.mean(torch.square(compute_gram(input_layers[layer]) -\n",
    "                                                    compute_gram(target_layers[layer])))\n",
    "    \n",
    "    return accumulated_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_cost(input):\n",
    "    tvloss = (\n",
    "        torch.sum(torch.abs(input[:, :, :, :-1] - input[:, :, :, 1:])) + \n",
    "        torch.sum(torch.abs(input[:, :, :-1, :] - input[:, :, 1:, :]))\n",
    "    )\n",
    "    return tvloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_losses = []\n",
    "style_losses = []\n",
    "tv_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_cost(input, targets):\n",
    "    # Weights\n",
    "    REG_TV = 1e-6\n",
    "    REG_STYLE = 1e6\n",
    "    REG_CONTENT = 50\n",
    "    \n",
    "    # Extract content and style images\n",
    "    content, style = targets\n",
    "    \n",
    "    # Get the content, style and tv variation losses\n",
    "    closs = content_cost(input, content) * REG_CONTENT\n",
    "    sloss = style_cost(input, style) * REG_STYLE\n",
    "    tvloss = total_variation_cost(input) * REG_TV\n",
    "        \n",
    "    # Add it to the running list of losses\n",
    "    content_losses.append(closs)\n",
    "    style_losses.append(sloss)\n",
    "    tv_losses.append(tvloss)\n",
    "    \n",
    "    print('****************************')\n",
    "    print('Content Loss: {}'.format(closs.item()))\n",
    "    print('Style Loss: {}'.format(sloss.item()))\n",
    "    print('Total Variation Loss: {}'.format(tvloss.item()))\n",
    "        \n",
    "    # Apply the weights and add\n",
    "    return closs + sloss + tvloss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_DIMENSIONS = (256, 256)\n",
    "DATA = list(glob.iglob('C:/PythonProjects/2year/10.DL_CV/data/train/images/*'))\n",
    "STYLE_IMAGE = np.asarray(Image.open('images/style/matrix.jpg').resize(IMG_DIMENSIONS)).transpose(2, 0, 1)[0:3]\n",
    "\n",
    "# Make the style image a batch and convert\n",
    "STYLE_IMAGE = STYLE_IMAGE.reshape(1, 3, 256, 256)\n",
    "TOTAL_DATA = len(DATA)\n",
    "MAX_TRAIN = int(TOTAL_DATA * 0.8)\n",
    "MAX_VAL = int(TOTAL_DATA * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_images():\n",
    "    f, axarr = plt.subplots(2,2)\n",
    "    \n",
    "    # Show colored images\n",
    "    axarr[0,0].imshow(np.asarray(Image.open(DATA[0]).resize(IMG_DIMENSIONS)))\n",
    "    axarr[0,1].imshow(np.asarray(Image.open(DATA[4]).resize(IMG_DIMENSIONS)))\n",
    "    axarr[1,0].imshow(np.asarray(Image.open(DATA[8]).resize(IMG_DIMENSIONS)))\n",
    "    \n",
    "    # Grayscale example\n",
    "    grayscale = np.asarray(Image.open(DATA[13]).resize(IMG_DIMENSIONS))\n",
    "    #grayscale = np.stack((grayscale, grayscale, grayscale)).transpose(1, 2, 0)\n",
    "    axarr[1,1].imshow(grayscale)\n",
    "    \n",
    "show_sample_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the chosen style image\n",
    "plt.imshow(STYLE_IMAGE[0].transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_batch(current_batch, batch_size, set_type):\n",
    "    \"\"\"\n",
    "    Load different batches of data (essentially a custom data loader for training, validation, and testing)\n",
    "    \"\"\"\n",
    "    # The initial position is where we want to start getting the batch\n",
    "    # So it is the starting index of the batch\n",
    "    initial_pos = current_batch * batch_size\n",
    "    \n",
    "    # List to store the images\n",
    "    images = []\n",
    "    \n",
    "    if set_type == 'train':\n",
    "        end_pos = min(initial_pos + batch_size, MAX_TRAIN) \n",
    "    elif set_type == 'val':\n",
    "        # Исправленная логика для валидационного набора\n",
    "        initial_pos = MAX_TRAIN + (current_batch * batch_size) \n",
    "        end_pos = min(initial_pos + batch_size, TOTAL_DATA) \n",
    "    elif set_type == 'test':\n",
    "        initial_pos = MAX_VAL + (current_batch * batch_size)\n",
    "        end_pos = min(initial_pos + batch_size, TOTAL_DATA)  \n",
    "\n",
    "    for f in DATA[initial_pos:initial_pos + batch_size]:\n",
    "        # Resize the image to 256 x 256\n",
    "        image = np.asarray(Image.open(f).resize(IMG_DIMENSIONS))\n",
    "        \n",
    "        # If the image is grayscale, stack the image 3 times to get 3 channels\n",
    "        if image.shape == IMG_DIMENSIONS:\n",
    "            image = np.stack((image, image, image))\n",
    "            images.append(image)\n",
    "            continue\n",
    "            \n",
    "        # Transpose the image to have channels first\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        images.append(image)\n",
    "    \n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_show_img():\n",
    "    # Get an image from the validation set\n",
    "    img = load_training_batch(0, 10, 'val')[4]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    train_img = torch.from_numpy(img.reshape(1, 3, 256, 256)).float().to(device)\n",
    "    \n",
    "    # Put through network\n",
    "    gen_img = transformation_net(train_img)\n",
    "    gen_img = gen_img.detach().cpu().numpy()\n",
    "    \n",
    "    # Clip the floats\n",
    "    gen_img = np.clip(gen_img, 0, 255)\n",
    "    \n",
    "    # Convert to ints (for images)\n",
    "    gen_img = gen_img.astype('uint8')\n",
    "    gen_img = gen_img.reshape(3, 256, 256).transpose(1, 2, 0)\n",
    "    \n",
    "    # Show the image\n",
    "    plt.imshow(gen_img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    # Convert to tensor\n",
    "    img = torch.from_numpy(img).float().to(device)\n",
    "    img = img.view(1, 3, 256, 256)\n",
    "    # Put through network\n",
    "    display(img.shape)\n",
    "    gen_img = transformation_net(img)\n",
    "    gen_img = gen_img.detach().cpu().numpy()\n",
    "    \n",
    "    # Clip the floats\n",
    "    gen_img = np.clip(gen_img, 0, 255)\n",
    "    \n",
    "    # Convert to ints (for images)\n",
    "    gen_img = gen_img.astype('uint8')\n",
    "    gen_img = gen_img.reshape(3, 256, 256).transpose(1, 2, 0)\n",
    "    \n",
    "    # Show the image\n",
    "    plt.imshow(gen_img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses():\n",
    "    # Print info about content losses\n",
    "    with torch.no_grad():\n",
    "        plt.plot([x.cpu() for x in content_losses[int(len(content_losses) * 0.0):]])\n",
    "        plt.show()\n",
    "        print(content_losses[len(content_losses) - 1])\n",
    "\n",
    "        # Print info about style losses\n",
    "        plt.plot([x.cpu() for x in style_losses[int(len(style_losses) * 0.0):]])\n",
    "        plt.show()\n",
    "        print(style_losses[len(style_losses) - 1])\n",
    "\n",
    "        # Print info about total variation losses\n",
    "        plt.plot([x.cpu() for x in tv_losses[int(len(tv_losses) * 0.0):]])\n",
    "        plt.show()\n",
    "        print(tv_losses[len(tv_losses) - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "STYLE_IMAGE_TENSOR = torch.from_numpy(np.copy(STYLE_IMAGE)).float()\n",
    "transformation_net = ImageTransformationNetwork().to(device)\n",
    "opt = optim.Adam(transformation_net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(2):\n",
    "    transformation_net.train()\n",
    "    for batch, _ in enumerate(range(0, MAX_TRAIN, BATCH_SIZE)):\n",
    "        # Skip what we've already done\n",
    "        if epoch == 0 and batch < (MAX_TRAIN/BATCH_SIZE):\n",
    "            continue\n",
    "        \n",
    "        # The content batch is the same as the train batch, except train batch has noise added to it\n",
    "        train_batch = load_training_batch(batch, BATCH_SIZE, 'train')\n",
    "        content_batch = np.copy(train_batch)\n",
    "\n",
    "        # Add noise to the training batch\n",
    "        train_batch = add_noise(train_batch)\n",
    "\n",
    "        # Convert the batches to tensors\n",
    "        train_batch = torch.from_numpy(train_batch).float().to(device)\n",
    "        content_batch = torch.from_numpy(content_batch).float().to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward propagate\n",
    "        gen_images = transformation_net(train_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = total_cost(gen_images, [content_batch, STYLE_IMAGE_TENSOR.to(device)])\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient to minimize chance of exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(transformation_net.parameters(), 1.0)\n",
    "\n",
    "        # Apply gradients\n",
    "        opt.step()\n",
    "\n",
    "        print(\"Training Batch: {}\".format(batch + 1), \"Loss: {:f}\".format(loss))\n",
    "        print('****************************')\n",
    "        \n",
    "        if batch % 100 == 99:\n",
    "            training_show_img()\n",
    "            plot_losses()\n",
    "        \n",
    "    transformation_net.eval()\n",
    "    for batch, _ in enumerate(range(MAX_TRAIN, MAX_VAL, BATCH_SIZE)):\n",
    "        # The content batch is the same as the train batch, except train batch has noise added to it\n",
    "        val_batch = load_training_batch(batch, BATCH_SIZE, 'val')\n",
    "        content_batch = np.copy(val_batch)\n",
    "        \n",
    "        # Add noise to the training batch\n",
    "        val_batch = add_noise(val_batch)\n",
    "        \n",
    "        # Convert the batches to tensors\n",
    "        val_batch = torch.from_numpy(val_batch).float().to(device)\n",
    "        content_batch = torch.from_numpy(content_batch).float().to(device)\n",
    "        \n",
    "        # Forward propagate\n",
    "        gen_images = transformation_net(val_batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = total_cost(gen_images, [content_batch, STYLE_IMAGE_TENSOR])\n",
    "        \n",
    "        print(\"Validation Batch: {}\".format(batch + 1), \"Loss: {:f}\".format(loss))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformation_net.state_dict(), 'models/matrix_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_show_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/matrix_2.pt\"\n",
    "transformation_net = ImageTransformationNetwork()\n",
    "transformation_net.load_state_dict(torch.load(model_path))\n",
    "transformation_net.to(device).eval()\n",
    "\n",
    "img = Image.open('images/mona_lisa.jpg')\n",
    "img = np.asarray(img.resize(IMG_DIMENSIONS)).transpose(2, 0, 1)[0:3]\n",
    "img = torch.from_numpy(img.reshape(1, 3, 256, 256)).float().to(device)\n",
    "\n",
    "# Вызовите модель для переноса стиля\n",
    "output_image = transformation_net(img)\n",
    "output_image = output_image.detach().cpu().numpy()\n",
    "output_image = np.clip(output_image, 0, 255)\n",
    "output_image = output_image.astype('uint8')\n",
    "output_image = output_image.reshape(3, 256, 256).transpose(1, 2, 0)\n",
    "\n",
    "plt.imshow(output_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.asarray(Image.open('test.jpg').resize(IMG_DIMENSIONS)).transpose(2, 0, 1)[0:3]\n",
    "show_img(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Конвертация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "model_path = \"models/matrix_2.pt\"\n",
    "transformation_net = ImageTransformationNetwork()\n",
    "transformation_net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "transformation_net.eval()\n",
    "\n",
    "example_input = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# Трассировка модели с помощью torch.jit.trace\n",
    "scripted_module = torch.jit.trace(transformation_net, example_input)\n",
    "#scripted_module.save(\"models/matrix_3_3.pt\")\n",
    "# Сохранение трассированной модели\n",
    "optimized_scripted_module = optimize_for_mobile(scripted_module)\n",
    "optimized_scripted_module._save_for_lite_interpreter(\"models/matrix_2.ptl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/matrix_2.pt\"\n",
    "transformation_net = ImageTransformationNetwork()\n",
    "transformation_net.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "\n",
    "# Установите модель в режим оценки\n",
    "transformation_net.eval()\n",
    "\n",
    "# Создайте пример входных данных для трассировки\n",
    "dummy_input = torch.randn(1, 3, 256, 256) # Пример, замените на ваши размерности\n",
    "\n",
    "# Трассировка модели и сохранение в формате TorchScript\n",
    "traced_model = torch.jit.trace(transformation_net, dummy_input)\n",
    "torch.jit.save(traced_model, \"models/matrix_2_1.ptl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
